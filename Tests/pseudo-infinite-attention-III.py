import os
import sys
from llama_cpp import Llama
import tiktoken
import pandas as pd


class PseudoInfiniteAttention:
    def __init__(self, model_path, context_length=2048, n_batch=512):
        """
        Initialize the model using llama-cpp-python

        :param model_path: Path to the .gguf model
        :param context_length: Maximum context length
        :param n_batch: Batch size for processing
        """
        # Check if model exists
        if not os.path.exists(model_path):
            print(f"Error: Model not found at {model_path}")
            sys.exit(1)

        # Initialize model with specified parameters
        self.model = Llama(
            model_path=model_path,
            n_ctx=context_length,  # Set context window size
            n_batch=n_batch,  # Set batch processing size
            verbose=False  # Disable verbose logging
        )

        # Use tiktoken for tokenization (OpenAI's tokenizer)
        self.tokenizer = tiktoken.get_encoding("cl100k_base")

    def tokenize(self, text):
        """
        Tokenize the input text

        :param text: Text to tokenize
        :return: List of tokens
        """
        return self.tokenizer.encode(text)

    def split_mindfile_into_fragments(self, mindfile, fragment_size=1000):
        """
        Split the mindfile into token fragments

        :param mindfile: Complete mindfile content
        :param fragment_size: Number of tokens per fragment
        :return: List of text fragments
        """
        tokens = self.tokenize(mindfile)
        fragments = []

        # Iterate through tokens with specified fragment size
        for i in range(0, len(tokens), fragment_size):
            fragment = self.tokenizer.decode(tokens[i:i + fragment_size])
            fragments.append(fragment)

        return fragments

    def process_fragment(self, fragment, user_message):
        """
        Process a single fragment of the mindfile

        :param fragment: Fragment of mindfile
        :param user_message: User's message
        :return: Generated response or 'PASS'
        """
        # Create a prompt with context and instructions
        prompt = (
            f"Note: we gave you only a small part of the data about the person. It's possible that your portion of "
            f"the data doesn't contain the necessary info. If it's the case, return 'PASS' "
            f"'{fragment} {user_message}'"
        )

        try:
            # Generate response using the model
            response = self.model(
                prompt,
                max_tokens=200,  # Limit response length
                stop=["PASS", "\n"],  # Stop generation conditions
                echo=False  # Don't echo the prompt
            )['choices'][0]['text'].strip()

            # Return response or 'PASS' if no relevant info
            return response if 'PASS' not in response else 'PASS'

        except Exception as e:
            print(f"Error processing fragment: {e}")
            return 'PASS'

    def filter_results(self, results, exclude='PASS'):
        """
        Filter out specific values from results

        :param results: List of results
        :param exclude: Value to exclude
        :return: Filtered list
        """
        return [result for result in results if result != exclude]

    def process_pseudo_infinite_attention(self, mindfile, user_message):
        """
        Process mindfile with pseudo-infinite attention mechanism

        :param mindfile: Complete mindfile path
        :param user_message: User's message
        :return: Final processed response and processing details
        """
        # Read mindfile content
        with open(mindfile, "r") as f:
            lines = f.readlines()
        text = "".join(lines)

        # Split mindfile into fragments
        fragments = self.split_mindfile_into_fragments(text)

        # Process each fragment and track details
        processing_details = []
        results = []

        for i, fragment in enumerate(fragments):
            response = self.process_fragment(fragment, user_message)
            processing_details.append({
                'Fragment': i + 1,
                'Tokens': len(self.tokenize(fragment)),
                'Response': response
            })
            results.append(response)

        # Filter valid results
        valid_results = self.filter_results(results)

        # Aggregation prompt to synthesize final response
        aggregation_prompt = (
            f"Below are the answers generated by several LLM instances. Each instance was provided with only a small "
            f"part of the data about the person. Your goal is to select the most relevant and comprehensive answer. "
            f"'PASS' indicates that the instance failed to find relevant info. "
            f"Answers: {' | '.join(valid_results)}"
        )

        # Generate final response
        final_response = self.model(
            aggregation_prompt,
            max_tokens=300,  # Limit final response length
            echo=False
        )['choices'][0]['text'].strip()

        return final_response, processing_details


def main():
    # Path to the model (adjust according to your installation)
    MODEL_PATH = "../Models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf"

    # Initialize Pseudo Infinite Attention
    pia = PseudoInfiniteAttention(MODEL_PATH)

    # Example usage
    mindfile = "MindFile.txt"
    user_message = "Hello Roman. What is your field of work?"

    # Process and get final response with processing details
    final_response, processing_details = pia.process_pseudo_infinite_attention(mindfile, user_message)

    # Print final synthesized response
    print("Synthesized Final Answer:", final_response)

    # Create and display processing details as a table
    details_df = pd.DataFrame(processing_details)
    print("\nProcessing Details:")
    print(details_df)


if __name__ == "__main__":
    main()